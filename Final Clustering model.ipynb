{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_excel('test_assignment.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6731, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(subset =\"String\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "String    6730\n",
       "Tag       1080\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>String</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3GPP TR 25.876 V7.0.0 (2007-03), Technical Rep...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              String  Tag\n",
       "0  3GPP Technical Specification Group, \"Spatial c...  1.0\n",
       "1  3GPP Technical Specification Group, \"Spatial c...  1.0\n",
       "2  3GPP Technical Specification Group, \"Spatial c...  1.0\n",
       "3  3GPP Technical Specification Group, \"Spatial c...  1.0\n",
       "4  3GPP TR 25.876 V7.0.0 (2007-03), Technical Rep...  2.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import datefinder\n",
    "from datetime import datetime\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from textblob import Word\n",
    "import re\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3GPP, \"LTE\", pages 1-63, April 2010 [online], Retrieved from the Internet:< URL:http://www.3gpp.org/LTE>.\n",
      "Tag: 8.0\n"
     ]
    }
   ],
   "source": [
    "def print_plot(index):\n",
    "    example = data[data.index == index][['String', 'Tag']].values[0]\n",
    "    print(example[0])\n",
    "    print('Tag:', example[1])\n",
    "print_plot(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z ]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "month_dict = dict(jan='01', feb='02', mar='03', apr='04', may='05', jun='06', jul='07', aug='08', sep='09',\n",
    "                  oct='10', nov='11', dec='12')\n",
    "def word_to_num(string):\n",
    "    #This function converts a string to lowercase and only accepts the first three letter.\n",
    "    s = string.lower()[:3]\n",
    "    return month_dict[s]\n",
    "\n",
    "def date_converter(text):\n",
    "    #This function extracts dates in every format from text and converts them to YYYYMMDD.\n",
    "    results = []\n",
    "    day = '01'\n",
    "    month = '01'\n",
    "    year = '1900'\n",
    "    regex = re.search('([0]?\\d|[1][0-2])[/-]([0-3]?\\d)[/-]([1-2]\\d{3}|\\d{2})', text)\n",
    "    # If format is DD Month YYYY or D Mon YY or some combination, also matches if no day given\n",
    "    month_regex = re.search(\n",
    "        '([0-3]?\\d)\\s*(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug('\n",
    "        '?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?\\s([1-2]\\d{3})',\n",
    "        text)\n",
    "    # If format is Month/DD/YYYY or Mon/D/YY or or Month DDth, YYYY or some combination\n",
    "    rev_month_regex = re.search(\n",
    "        '(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct('\n",
    "        '?:ober)?|Nov(?:ember)?|Dec(?:ember)?).?[-\\s]([0-3]?\\d)(?:st|nd|rd|th)?[-,\\s]\\s*([1-2]\\d{3})',\n",
    "        text)\n",
    "    # If format is any combination of just Month or Mon and YY or YYYY\n",
    "    no_day_regex = re.search(\n",
    "        '(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?('\n",
    "        '?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?[\\s]([1-2]\\d{3}|\\d{2})',\n",
    "        text)\n",
    "    # If format is MM/YYYY or M YYYY or some combination\n",
    "    no_day_digits_regex = re.search('([0]?\\d|[1][0-2])[/\\s]([1-2]\\d{3})', text)\n",
    "    # If format only contains a year. If year is written alone it must be in form YYYY\n",
    "    year_only_regex = re.search('([1-2]\\d{3})', text)\n",
    "    if regex:\n",
    "        day = regex.group(2)\n",
    "        month = regex.group(1)\n",
    "        year = regex.group(3)\n",
    "    elif month_regex:\n",
    "        day = month_regex.group(1)\n",
    "        month = word_to_num(month_regex.group(2))\n",
    "        year = month_regex.group(3)\n",
    "    elif rev_month_regex:\n",
    "        day = rev_month_regex.group(2)\n",
    "        month = word_to_num(rev_month_regex.group(1))\n",
    "        year = rev_month_regex.group(3)\n",
    "    elif no_day_regex:\n",
    "        month = word_to_num(no_day_regex.group(1))\n",
    "        year = no_day_regex.group(2)\n",
    "    elif no_day_digits_regex:\n",
    "        month = no_day_digits_regex.group(1)\n",
    "        year = no_day_digits_regex.group(2)\n",
    "    elif year_only_regex:\n",
    "        year = year_only_regex.group(0)\n",
    "    # Make sure all variables have correct number, add zeros if necessary\n",
    "    month = month.zfill(2)\n",
    "    day = day.zfill(2)\n",
    "    if day == '00':\n",
    "        day = '01'\n",
    "    if year is not None and len(year) == 2:\n",
    "        year = '19' + year\n",
    "    results.append(year+'-'+month+'-'+day)\n",
    "    return results\n",
    "\n",
    "def unique_list(l):\n",
    "    ulist = []\n",
    "    [ulist.append(x) for x in l if x not in ulist]\n",
    "    return ulist\n",
    "\n",
    "def clean_text(text):        \n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    dt = date_converter(text)\n",
    "    text = text.lower() # lowercase text\n",
    "    text = re.sub('retrieved from the internet', '', text)\n",
    "    text = re.sub('online', '', text)\n",
    "    text = re.sub('url', '', text)\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    te = re.findall(r'\"(.*?)\"', text)\n",
    "    te = ''.join(te)\n",
    "    te =' '.join(unique_list(te.split()))\n",
    "    #te = ''.join(te.split())\n",
    "    text = re.sub('- ', '', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    te = BAD_SYMBOLS_RE.sub('', te)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    text =' '.join(unique_list(text.split()))\n",
    "    text = text+' '+te\n",
    "    dt = ''.join([str(elem) for elem in dt])\n",
    "    text = text+' '+str(dt)\n",
    "    return text\n",
    "   \n",
    "data['Conv_String'] = data['String'].apply(clean_text)\n",
    "data['Conv_String'] = data['Conv_String'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3GPP TR 25.913, \"Requirements for Evolved UTRA (E-UTRA) and Evolved UTRAN (E-UTRAN)\", V8.0.0 (Jan. 2009).\n"
     ]
    }
   ],
   "source": [
    "print(data['String'][35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3gpp tr 25913 requirement evolved utra eutra utran eutran v800 jan 2009 requirement for evolved utra eutra and utran eutran 2009-01-01\n"
     ]
    }
   ],
   "source": [
    "print(data['Conv_String'][35])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data, columns=['String','Conv_String','Tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>String</th>\n",
       "      <th>Conv_String</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>3gpp technical specification group spatial cha...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>3gpp technical specification group spatial cha...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>3gpp technical specification group spatial cha...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>3gpp technical specification group spatial cha...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3GPP TR 25.876 V7.0.0 (2007-03), Technical Rep...</td>\n",
       "      <td>3gpp tr 25876 v700 200703 technical report 3rd...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              String  \\\n",
       "0  3GPP Technical Specification Group, \"Spatial c...   \n",
       "1  3GPP Technical Specification Group, \"Spatial c...   \n",
       "2  3GPP Technical Specification Group, \"Spatial c...   \n",
       "3  3GPP Technical Specification Group, \"Spatial c...   \n",
       "4  3GPP TR 25.876 V7.0.0 (2007-03), Technical Rep...   \n",
       "\n",
       "                                         Conv_String  Tag  \n",
       "0  3gpp technical specification group spatial cha...  1.0  \n",
       "1  3gpp technical specification group spatial cha...  1.0  \n",
       "2  3gpp technical specification group spatial cha...  1.0  \n",
       "3  3gpp technical specification group spatial cha...  1.0  \n",
       "4  3gpp tr 25876 v700 200703 technical report 3rd...  2.0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152367"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Conv_String'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.Conv_String"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(analyzer='word', token_pattern=r'\\w{1,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_count = count_vect.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tfidf = tfidf_vect.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6730, 3995)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6730, 3995)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_count.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "from matplotlib import style\n",
    "from sklearn.datasets.samples_generator import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6730"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = data.Conv_String.shape[0]\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322]\n",
      "220\n"
     ]
    }
   ],
   "source": [
    "c=m//3\n",
    "d=m//6\n",
    "ls1 = [*range(c-40,c+80)]\n",
    "ls2 = [*range(d-50,d+50)]\n",
    "ls = list(set(ls1+ls2))\n",
    "print(ls)\n",
    "print(len(ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = [1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nbest_clusters = 0                       \\nprevious_silh_avg = 0.0\\n\\nfor n_clusters in ls:\\n    clusterer = KMeans(n_clusters=n_clusters,init='k-means++', n_jobs=-2)\\n    cluster_labels = clusterer.fit_predict(x_tfidf)\\n    silhouette_avg = silhouette_score(x_tfidf, cluster_labels)\\n    if silhouette_avg > previous_silh_avg:\\n        previous_silh_avg = silhouette_avg\\n        best_clusters = n_clusters\\n\\n# Final Kmeans for best_clusters\\nprint(best_clusters)\\n\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "best_clusters = 0                       \n",
    "previous_silh_avg = 0.0\n",
    "\n",
    "for n_clusters in ls:\n",
    "    clusterer = KMeans(n_clusters=n_clusters,init='k-means++', n_jobs=-2)\n",
    "    cluster_labels = clusterer.fit_predict(x_tfidf)\n",
    "    silhouette_avg = silhouette_score(x_tfidf, cluster_labels)\n",
    "    if silhouette_avg > previous_silh_avg:\n",
    "        previous_silh_avg = silhouette_avg\n",
    "        best_clusters = n_clusters\n",
    "\n",
    "# Final Kmeans for best_clusters\n",
    "print(best_clusters)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clusters=1080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=best_clusters, \n",
    "               init='k-means++', \n",
    "               max_iter=100, # Maximum number of iterations of the k-means algorithm for a single run.\n",
    "               n_init=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=100,\n",
       "       n_clusters=1080, n_init=1, n_jobs=None, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[970 970 970 ... 811 811 811]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = tfidf_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3gpp tr 25913 requirement evolved utra eutra utran eutran v800 jan 2009 pp 120 requirement for evolved utra eutra and utran eutran 2009-01-01\n"
     ]
    }
   ],
   "source": [
    "test = data.Conv_String[28]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3gpp technical specification group spatial channel model scm134 text v60 ahg combined adhoc 3gpp2 april 2003 pp 145 spatial channel model scm134 text v60 2003-04-01\n"
     ]
    }
   ],
   "source": [
    "test = data.Conv_String[1]\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text belongs to cluster number 970\n"
     ]
    }
   ],
   "source": [
    "x_t = tfidf_vect.transform([test])\n",
    "\n",
    "cluster = model.predict(x_t)[0]\n",
    "\n",
    "print(\"Text belongs to cluster number {0}\".format(cluster))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_tfidf, labels, test_size=0.2, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishek.tyagi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\abhishek.tyagi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_lr.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7919762258543833\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>String</th>\n",
       "      <th>Conv_String</th>\n",
       "      <th>Tag</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>3gpp technical specification group spatial cha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>3gpp technical specification group spatial cha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>3gpp technical specification group spatial cha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3GPP Technical Specification Group, \"Spatial c...</td>\n",
       "      <td>3gpp technical specification group spatial cha...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3GPP TR 25.876 V7.0.0 (2007-03), Technical Rep...</td>\n",
       "      <td>3gpp tr 25876 v700 200703 technical report 3rd...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              String  \\\n",
       "0  3GPP Technical Specification Group, \"Spatial c...   \n",
       "1  3GPP Technical Specification Group, \"Spatial c...   \n",
       "2  3GPP Technical Specification Group, \"Spatial c...   \n",
       "3  3GPP Technical Specification Group, \"Spatial c...   \n",
       "4  3GPP TR 25.876 V7.0.0 (2007-03), Technical Rep...   \n",
       "\n",
       "                                         Conv_String  Tag  Cluster  \n",
       "0  3gpp technical specification group spatial cha...  1.0      970  \n",
       "1  3gpp technical specification group spatial cha...  1.0      970  \n",
       "2  3gpp technical specification group spatial cha...  1.0      970  \n",
       "3  3gpp technical specification group spatial cha...  1.0      970  \n",
       "4  3gpp tr 25876 v700 200703 technical report 3rd...  2.0      415  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('data_clustering_3.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style.use(\"fivethirtyeight\") \n",
    "\n",
    "cost =[] \n",
    "for i in range(1077, 1082):\n",
    "    KM = KMeans(n_clusters = i, max_iter = 500)\n",
    "    KM.fit(x_count)\n",
    "    cost.append(KM.inertia_)\n",
    "\n",
    "# plot the cost against K values \n",
    "plt.plot(range(1077, 1082), cost, color ='g', linewidth ='3') \n",
    "plt.xlabel(\"Value of K\") \n",
    "plt.ylabel(\"Sqaured Error (Cost)\") \n",
    "plt.show() # clear the plot \n",
    "\n",
    "# the point of the elbow is the \n",
    "# most optimal value for choosing k "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_WSS(points, kmax):\n",
    "  sse = []\n",
    "  for k in range(1, kmax+1):\n",
    "    kmeans = KMeans(n_clusters = k).fit(points)\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    pred_clusters = kmeans.predict(points)\n",
    "    curr_sse = 0\n",
    "    \n",
    "    # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
    "    for i in range(len(points)):\n",
    "      curr_center = centroids[pred_clusters[i]]\n",
    "      curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
    "      \n",
    "    sse.append(curr_sse)\n",
    "  return sse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(messages)\n",
    "    kmeanModel.fit(messages)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(m):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = KMeans(points)\n",
    "for t in range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters=4, init='k-means++', max_iter=100, n_init=1)\n",
    "model.fit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = train_test_split(x, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = \"calvin klein design dress calvin klein\"\n",
    "string2 = ' '.join(set(string1.split()))\n",
    "print(string2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z ]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def unique_list(l):\n",
    "    ulist = []\n",
    "    [ulist.append(x) for x in l if x not in ulist]\n",
    "    return ulist\n",
    "\n",
    "def clean_text(text):\n",
    "    text = BeautifulSoup(text, \"lxml\").text # HTML decoding\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    te = re.findall(r'\"(.*?)\"', text)\n",
    "    te = ''.join(te)\n",
    "    te =' '.join(unique_list(te.split()))\n",
    "    te = ''.join(te.split())\n",
    "    text = re.sub('- ', '', text)\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    te = BAD_SYMBOLS_RE.sub('', te)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # delete stopwors from text\n",
    "    text =' '.join(unique_list(text.split()))\n",
    "    text = text+' '+te\n",
    "    return text\n",
    "    \n",
    "data['Conv_String'] = data['String'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\", \"...\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) new_text = \" \".join(new_words) \n",
    "        return new_text\n",
    "\n",
    "_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")\n",
    ">> \"Retweet this is a retweeted tweet by Shivam Bansal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06313193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhishek.tyagi\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "# train the model on your corpus  \n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "\n",
    "print(model.similarity('data', 'science'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "train['tweet'][:5].apply(lambda x: str(TextBlob(x).correct()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt= '3gpp tr 25913 requirement evolved utra eutra utran eutran v800 jan 2009 pp 120 requirement for evolved utra eutra and utran eutran'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = '3GPP TR 25.913, \"Requirements for Evolved UTRA (E-UTRA) and Evolved UTRAN (E-UTRAN)\", V8.0.0 ( Jan. 2009), pp. 1-20.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('913', datetime.datetime(1900, 1, 1, 9, 1, 3)), ('Jan', datetime.datetime(1900, 1, 1, 0, 0)), ('2009', datetime.datetime(2009, 1, 1, 0, 0)), ('1-20', datetime.datetime(2009, 1, 20, 0, 0))]\n"
     ]
    }
   ],
   "source": [
    "from dateparser.search import search_dates\n",
    "dates = search_dates(dt1)\n",
    "print(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[datetime.datetime(2009, 1, 29, 0, 0)]\n",
      "[datetime.datetime(2020, 1, 20, 0, 0)]\n"
     ]
    }
   ],
   "source": [
    "import datefinder\n",
    "import parsedatetime\n",
    "from datetime import datetime\n",
    "from dateparser.search import search_dates\n",
    "\n",
    "ls=[]\n",
    "matches = datefinder.find_dates(dt1)\n",
    "for match in matches:\n",
    "    print([match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "match = re.search(r'\\d{4}-\\d{2}-\\d{2}', text)\n",
    "date = datetime.strptime(match.group(), '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "strptime() argument 1 must be str, not datetime.datetime",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-125-75a1ceb7feb8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatefinder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_dates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mdate_time_obj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: strptime() argument 1 must be str, not datetime.datetime"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "matches = datefinder.find_dates(dt1)\n",
    "for match in matches:\n",
    "    date_time_obj = datetime.datetime.strptime(match, '%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "all = re.findall(r\"[\\d]{1,2} [ADFJMNOS]\\w* [\\d]{4}\", dt1)\n",
    "print(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 12, 1, 0, 0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dateutil.parser import parse\n",
    "result = parse(\"Today is 12-01-18\", fuzzy_with_tokens=True)\n",
    " \n",
    "# get just the datetime object\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from datetime import date\n",
    "import re\n",
    "s = \"Jason's birthday is on 1991-09-21\"\n",
    "match = re.search(r'\\d{4}-\\d{2}-\\d{2}', s)\n",
    "date = datetime.datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "print date\n",
    "import re\n",
    "m = re.search('(?<=-)\\w+', 'derer-10-12-2001.zip')\n",
    "print m.group(0)\n",
    "re.search(\"([0-9]{2}\\-[0-9]{2}\\-[0-9]{4})\", fileName)\n",
    "m = re.search('\\b(\\d{2}-\\d{2}-\\d{4})\\.', 'derer-10-12-2001.zip')\n",
    "print m.group(1)\n",
    "re.search(r'(?<=-)[\\d-]+(?=\\.)', name).group(0)\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "match = re.search(r'\\d{4}-\\d{2}-\\d{2}', text)\n",
    "date = datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "In [1]: import dateutil.parser as dparser\n",
    "\n",
    "In [18]: dparser.parse(\"monkey 2010-07-10 love banana\",fuzzy=True)\n",
    "Out[18]: datetime.datetime(2010, 7, 10, 0, 0)\n",
    "    import datefinder\n",
    "\n",
    "input_string = \"monkey 2010-07-10 love banana\"\n",
    "# a generator will be returned by the datefinder module. I'm typecasting it to a list. Please read the note of caution provided at the bottom.\n",
    "matches = list(datefinder.find_dates(input_string))\n",
    "\n",
    "if len(matches) > 0:\n",
    "    # date returned will be a datetime.datetime object. here we are only using the first match.\n",
    "    date = matches[0]\n",
    "    print date\n",
    "else:\n",
    "    print 'No dates found'\n",
    "    \n",
    "from pygrok import Grok\n",
    "\n",
    "input_string = 'monkey 2010-07-10 love banana'\n",
    "date_pattern = '%{YEAR:year}-%{MONTHNUM:month}-%{MONTHDAY:day}'\n",
    "\n",
    "grok = Grok(date_pattern)\n",
    "print(grok.match(input_string))\n",
    "fmt_string2 = I want to apply for leaves from 12/12/2017 to 12/18/2017\n",
    "''.join(fmt_string2.split()[-1].split('.')[::-10])\n",
    "import re\n",
    "\n",
    "string = 'I want to apply for leaves from 12/12/2017 to 12/18/2017 I want to apply for leaves from 12 January 2017 to ' \\\n",
    "       '12/18/2017 I want to apply for leaves from 12/12/2017 to 12 Jan 17 '\n",
    "\n",
    "matches = re.findall('(\\d{2}[\\/ ](\\d{2}|January|Jan|February|Feb|March|Mar|April|Apr|May|May|June|Jun|July|Jul|August|Aug|September|Sep|October|Oct|November|Nov|December|Dec)[\\/ ]\\d{2,4})', string)\n",
    "\n",
    "for match in matches:\n",
    "    print(match[0])\n",
    "pattern = re.compile(r'from (.*) to (.*)')    \n",
    "matches = re.findall(pattern, text)\n",
    "for val in matches:\n",
    "    try:\n",
    "        dt_from = parse(val[0])\n",
    "        dt_to = parse(val[1])\n",
    "\n",
    "        print(\"Leave applied from\", dt_from.strftime('%d/%b/%Y'), \"to\", dt_to.strftime('%d/%b/%Y'))\n",
    "    except ValueError:\n",
    "        print(\"skipping\", val)\n",
    "months_list= []\n",
    "for month_idx in range(1, 13):\n",
    "    months_list.append(calendar.month_name[month_idx])\n",
    "    months_list.append(calendar.month_abbr[month_idx])\n",
    "\n",
    "# join the list to use it as pyparsing keyword\n",
    "month_keywords = \" \".join(months_list)\n",
    "# date separator - can be one of '/', '.', or ' '\n",
    "separator = pp.Word(\"/. \")\n",
    "\n",
    "# Dictionary for numeric date e.g. 12/12/2018\n",
    "numeric_date = pp.Combine(pp.Word(pp.nums, max=2) + separator + pp.Word(pp.nums, max=2) + separator + pp.Word(pp.nums, max=4))\n",
    "\n",
    "# Dictionary for text date e.g. 12/Jan/2018\n",
    "text_date = pp.Combine(pp.Word(pp.nums, max=2) + separator + pp.oneOf(month_keywords) + separator + pp.Word(pp.nums, max=4))\n",
    "\n",
    "# Either numeric or text date\n",
    "date_pattern = numeric_date | text_date\n",
    "\n",
    "# Final dictionary - from x to y\n",
    "pattern = pp.Suppress(pp.SkipTo(\"from\") + pp.Word(\"from\") + pp.Optional(\"start\") + pp.Optional(\"date\")) + date_pattern\n",
    "pattern += pp.Suppress(pp.Word(\"to\") + pp.Optional(\"end\") + pp.Optional(\"date\")) + date_pattern\n",
    "\n",
    "# Group the pattern, also it can be multiple\n",
    "pattern = pp.OneOrMore(pp.Group(pattern))\n",
    "result = pattern.parseString(text)\n",
    "\n",
    "# Print result\n",
    "for match in result:\n",
    "    print(\"from\", match[0], \"to\", match[1])\n",
    "exp_date = re.findall(r'exp\\w+ date[ :]*\\d+[ -/]\\d+[ -/]\\d+',w.text,re.IGNORECASE)\n",
    "date, month = re.search(\"^Date:[^,]+,\\ (\\d+) (\\w+)\", Data, re.MULTILINE).groups()\n",
    "date = int(date)\n",
    "\n",
    "print(date, month)\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "data1 = re.sub(' ', '', data)\n",
    "res = re.search(r'Date(.*)$', data1, re.MULTILINE).group()\n",
    "res2 = datetime.strptime(res, 'Date:%a,%d%b%Y%X%z')\n",
    "print(res2.day, res2.month)\n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "\n",
    "date = re.search(\"Date(.*)$\", s, re.MULTILINE)\n",
    "if date:\n",
    "    date = date.group().replace(\"Date:\", \"\").strip()\n",
    "    d = parse(date)\n",
    "    Date = d.day\n",
    "    Month = d.strftime(\"%b\")\n",
    "print(Date, Month)\n",
    "import re\n",
    "s = \"\"\"Registrar Registration Expiration Date: 10/4/2018\n",
    "Expiry date: 10/4/2018 \"\"\"\n",
    "\n",
    "print(re.findall('Expiration Date:*(.+)|Expiry Date:*(.+)', s, re.IGNORECASE))\n",
    " fmt_string2 = I want to apply for leaves from 12/12/2017 to 12/18/2017\n",
    "''.join(fmt_string2.split()[-1].split('.')[::-10])\n",
    "import re\n",
    "\n",
    "string = 'I want to apply for leaves from 12/12/2017 to 12/18/2017 I want to apply for leaves from 12 January 2017 to ' \\\n",
    "       '12/18/2017 I want to apply for leaves from 12/12/2017 to 12 Jan 17 '\n",
    "\n",
    "matches = re.findall('(\\d{2}[\\/ ](\\d{2}|January|Jan|February|Feb|March|Mar|April|Apr|May|May|June|Jun|July|Jul|August|Aug|September|Sep|October|Oct|November|Nov|December|Dec)[\\/ ]\\d{2,4})', string)\n",
    "\n",
    "for match in matches:\n",
    "    print(match[0])\n",
    "pattern = re.compile(r'from (.*) to (.*)')    \n",
    "matches = re.findall(pattern, text)\n",
    "for val in matches:\n",
    "    try:\n",
    "        dt_from = parse(val[0])\n",
    "        dt_to = parse(val[1])\n",
    "\n",
    "        print(\"Leave applied from\", dt_from.strftime('%d/%b/%Y'), \"to\", dt_to.strftime('%d/%b/%Y'))\n",
    "    except ValueError:\n",
    "        print(\"skipping\", val)\n",
    "months_list= []\n",
    "for month_idx in range(1, 13):\n",
    "    months_list.append(calendar.month_name[month_idx])\n",
    "    months_list.append(calendar.month_abbr[month_idx])\n",
    "\n",
    "# join the list to use it as pyparsing keyword\n",
    "month_keywords = \" \".join(months_list)\n",
    "# date separator - can be one of '/', '.', or ' '\n",
    "separator = pp.Word(\"/. \")\n",
    "\n",
    "# Dictionary for numeric date e.g. 12/12/2018\n",
    "numeric_date = pp.Combine(pp.Word(pp.nums, max=2) + separator + pp.Word(pp.nums, max=2) + separator + pp.Word(pp.nums, max=4))\n",
    "\n",
    "# Dictionary for text date e.g. 12/Jan/2018\n",
    "text_date = pp.Combine(pp.Word(pp.nums, max=2) + separator + pp.oneOf(month_keywords) + separator + pp.Word(pp.nums, max=4))\n",
    "\n",
    "# Either numeric or text date\n",
    "date_pattern = numeric_date | text_date\n",
    "\n",
    "# Final dictionary - from x to y\n",
    "pattern = pp.Suppress(pp.SkipTo(\"from\") + pp.Word(\"from\") + pp.Optional(\"start\") + pp.Optional(\"date\")) + date_pattern\n",
    "pattern += pp.Suppress(pp.Word(\"to\") + pp.Optional(\"end\") + pp.Optional(\"date\")) + date_pattern\n",
    "\n",
    "# Group the pattern, also it can be multiple\n",
    "pattern = pp.OneOrMore(pp.Group(pattern))\n",
    "result = pattern.parseString(text)\n",
    "\n",
    "# Print result\n",
    "for match in result:\n",
    "    print(\"from\", match[0], \"to\", match[1])\n",
    "(\\d{1,4}([.\\-/])\\d{1,2}([.\\-/])\\d{1,4})\n",
    "all = re.findall(r\"[\\d]{1,2}/[\\d]{1,2}/[\\d]{4}\", str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "test_cases = ['04/30/2009', '06/20/95', '8/2/69', '1/25/2011', '9/3/2002', '4-13-82', 'Mar-02-2009', 'Jan 20, 1974',\n",
    "        'March 20, 1990', 'Dec. 21, 2001', 'May 25 2009', '01 Mar 2002', '2 April 2003', '20 Aug. 2004',\n",
    "        '20 November, 1993', 'Aug 10th, 1994', 'Sept 1st, 2005', 'Feb. 22nd, 1988', 'Sept 2002', 'Sep 2002',\n",
    "        'December, 1998', 'Oct. 2000', '6/2008', '12/2001', '1998', '2002']\n",
    "\n",
    "# Create a dictionary to convert from month names to numbers (e.g. Jan = 01)\n",
    "month_dict = dict(jan='01', feb='02', mar='03', apr='04', may='05', jun='06', jul='07', aug='08', sep='09',\n",
    "                  oct='10', nov='11', dec='12')\n",
    "\n",
    "\n",
    "def word_to_num(string):\n",
    "    \"\"\"\n",
    "    This function converts a string to lowercase and only accepts the first three letter.\n",
    "    This is to prepare a string for month_dict\n",
    "    Example:\n",
    "        word_to_num('January') -> jan\n",
    "    \"\"\"\n",
    "\n",
    "    s = string.lower()[:3]\n",
    "    return month_dict[s]\n",
    "\n",
    "\n",
    "def date_converter(line):\n",
    "    \"\"\"\n",
    "    This function extracts dates in every format from text and converts them to YYYYMMDD.\n",
    "    Example:\n",
    "        date_converter(\"It was the May 1st, 2009\") -> 20090501\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    day = '01'\n",
    "    month = '01'\n",
    "    year = '1900'\n",
    "    # If format is MM/DD/YYYY or M/D/YY or some combination\n",
    "    regex = re.search('([0]?\\d|[1][0-2])[/-]([0-3]?\\d)[/-]([1-2]\\d{3}|\\d{2})', line)\n",
    "    # If format is DD Month YYYY or D Mon YY or some combination, also matches if no day given\n",
    "    month_regex = re.search(\n",
    "        '([0-3]?\\d)\\s*(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug('\n",
    "        '?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?\\s([1-2]\\d{3})',\n",
    "        line)\n",
    "    # If format is Month/DD/YYYY or Mon/D/YY or or Month DDth, YYYY or some combination\n",
    "    rev_month_regex = re.search(\n",
    "        '(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct('\n",
    "        '?:ober)?|Nov(?:ember)?|Dec(?:ember)?).?[-\\s]([0-3]?\\d)(?:st|nd|rd|th)?[-,\\s]\\s*([1-2]\\d{3})',\n",
    "        line)\n",
    "    # If format is any combination of just Month or Mon and YY or YYYY\n",
    "    no_day_regex = re.search(\n",
    "        '(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?('\n",
    "        '?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?[\\s]([1-2]\\d{3}|\\d{2})',\n",
    "        line)\n",
    "    # If format is MM/YYYY or M YYYY or some combination\n",
    "    no_day_digits_regex = re.search('([0]?\\d|[1][0-2])[/\\s]([1-2]\\d{3})', line)\n",
    "    # If format only contains a year. If year is written alone it must be in form YYYY\n",
    "    year_only_regex = re.search('([1-2]\\d{3})', line)\n",
    "    if regex:\n",
    "        day = regex.group(2)\n",
    "        month = regex.group(1)\n",
    "        year = regex.group(3)\n",
    "    elif month_regex:\n",
    "        day = month_regex.group(1)\n",
    "        month = word_to_num(month_regex.group(2))\n",
    "        year = month_regex.group(3)\n",
    "    elif rev_month_regex:\n",
    "        day = rev_month_regex.group(2)\n",
    "        month = word_to_num(rev_month_regex.group(1))\n",
    "        year = rev_month_regex.group(3)\n",
    "    elif no_day_regex:\n",
    "        month = word_to_num(no_day_regex.group(1))\n",
    "        year = no_day_regex.group(2)\n",
    "    elif no_day_digits_regex:\n",
    "        month = no_day_digits_regex.group(1)\n",
    "        year = no_day_digits_regex.group(2)\n",
    "    elif year_only_regex:\n",
    "        year = year_only_regex.group(0)\n",
    "    # Make sure all variables have correct number, add zeros if necessary\n",
    "    month = month.zfill(2)\n",
    "    day = day.zfill(2)\n",
    "    if day == '00':\n",
    "        day = '01'\n",
    "    if year is not None and len(year) == 2:\n",
    "        year = '19' + year\n",
    "    results.append(year + month + day)\n",
    "    return results\n",
    "\n",
    "\n",
    "test_run = [date_converter(w) for w in test_cases]\n",
    "print(test_run)\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "test_cases = ['04/30/2009', '06/20/95', '8/2/69', '1/25/2011', '9/3/2002', '4-13-82', 'Mar-02-2009', 'Jan 20, 1974',\n",
    "              'March 20, 1990', 'Dec. 21, 2001', 'May 25 2009', '01 Mar 2002', '2 April 2003', '20 Aug. 2004',\n",
    "              '20 November, 1993', 'Aug 10th, 1994', 'Sept 1st, 2005', 'Feb. 22nd, 1988', 'Sept 2002', 'Sep 2002',\n",
    "              'December, 1998', 'Oct. 2000', '6/2008', '12/2001', '1998', '2002']\n",
    "\n",
    "for date_string in test_cases:\n",
    "    print(date_string, parse(date_string).strftime(\"%Y%m%d\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
